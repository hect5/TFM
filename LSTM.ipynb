{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "MnWR21SmsYZA",
        "coKIy8NPLafc",
        "jo9lwgCRukcg",
        "alMCOAE4VK9u",
        "mQ7lR1NkafmY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heketor/TFN/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLsqi5jKuSuM",
        "colab_type": "text"
      },
      "source": [
        "# LSTM "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt90iPF5c0VL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Utilizar mi dataset (enlaces de trafico), y con la LSTM predecir como va ser el trafico. Si aumenta la carga de trafico, poner mas recursos (bandwidth), con esto se resolveria en un prinpio el problema de *resource allocation*. Por otro lado utilizar DDLO pero en vez de pasarle el task_size. se va a alimentar con los datos anteriores para realizar la decision de offloading. \n",
        "\n",
        "Reduce delay.\n",
        "\n",
        "Tengo que fijarme en que en DDLO le meto un vector de task size, eso me devuelve 3 vectore con las decisiones tomadas de las 3 redes. y computo Q para decidr cual es mejor y es la que le meto junto a es task size para entrenar\n",
        "\n",
        "En vez de meterle a cara perro decisiones de offloading, podria ser una de los motivos por los que la red no esta aprendiendo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kasIR-fbvwC",
        "colab_type": "text"
      },
      "source": [
        "### Important links \n",
        "\n",
        "[Pythorch Module Documentation](https://pytorch.org/docs/stable/nn.html#module)\n",
        "\n",
        "[Pytorch nn tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html )\n",
        "\n",
        "[Pytorch Tutorial](https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/)\n",
        "\n",
        "[Colab Tutorial](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d )\n",
        "\n",
        "[ Understanding LSTMs ](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "[Tensorboard in Pytorch Colab I](https://colab.research.google.com/github/SapanaChaudhary/Colab-pages/blob/master/Tensorboard_for_PyTorch.ipynb#scrollTo=aWV1w1AvLfiN)\n",
        "\n",
        "[Tensorboard in Pytorch in Colab II](https://medium.com/logojoy-engineering/how-to-use-tensorboard-with-pytorch-in-google-colab-1f76a938bc34)\n",
        "\n",
        "[Pytorch codes!!](https://github.com/yunjey/pytorch-tutorial)\n",
        "\n",
        "[Activation Functions](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
        "\n",
        "**Mount Drive**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPHQDGgNQRjN",
        "colab_type": "code",
        "outputId": "e14d14e3-ba38-42b5-b406-8fbb310ea1c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "#!ls -l \"/content/drive/My Drive/TFM/RCLSTM/\"\n",
        "#!chmod 777 \"/content/drive/My Drive/TFM/RCLSTM/rclstm.py\"\n",
        "\n",
        "\n",
        "#!cp \"/content/drive/My Drive/TFM/RCLSTM/rclstm.py\" \"/content/rclstm.py\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnWR21SmsYZA",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIzVkF1BAIX1",
        "colab_type": "text"
      },
      "source": [
        "**KERAS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcQGTzbXsX-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ngrok executable can be directly downloaded to your Colab notebook\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "#TensorBoard in the background. It is assuming the TensorBoard log path is \"./log\"\n",
        "It is assuming the TensorBoard log path is \"./log\"\n",
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "#Run ngrok to tunnel TensorBoard port 6006 to the outside world\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "#Get the public URL where we can access the colab TensorBoard web page\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qceI0ThyAMHg",
        "colab_type": "text"
      },
      "source": [
        "**PYTORCH**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAVNfeccX5Hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install latest Tensorflow build\n",
        "!pip install -q tf-nightly-2.0-preview\n",
        "\n",
        "# #Instantiate summay writers\n",
        "# current_time = str(datetime.now().timestamp())\n",
        "# train_log_dir = '/content/drive/My Drive/TFM/RCLSTM/logs/tensorboard/train/' + current_time\n",
        "# test_log_dir = '/content/drive/My Drive/TFM/RCLSTM/logs/tensorboard/test/' + current_time\n",
        "# train_summary_writer = summary.create_file_writer(train_log_dir)\n",
        "# test_summary_writer = summary.create_file_writer(test_log_dir)\n",
        "\n",
        "#Live Tensorboard\n",
        "\n",
        "%tensorboard --logdir '/content/drive'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coKIy8NPLafc",
        "colab_type": "text"
      },
      "source": [
        "## Data proccessing\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bZ0CZGALZSN",
        "colab_type": "code",
        "outputId": "31c6fab2-4b72-4b58-db9c-fc3c25197f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# encoding: utf-8\n",
        "\n",
        "import numpy as np \n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "save_datetime = True\n",
        "save_traffic = False\n",
        "save_normalized = False\n",
        "\n",
        "num_day = 10772  # numero de arhivos\n",
        "num_ids = 23     # filas y columnas del csv\n",
        "# path of csv files\n",
        "dst_path = '/content/drive/My Drive/TFM/RCLSTM/data/csv/'\n",
        "# initialize traffic matrix\n",
        "Traffic_matrices = np.empty([num_day, num_ids, num_ids])\n",
        "# initialize normalized traffic matrix\n",
        "normalized_traffic_matrices = np.empty([num_day, num_ids, num_ids])\n",
        "#Save the dictionary list of datetime\n",
        "date_time = []\n",
        "\n",
        "# get the traffic from point 1 to point 2\n",
        "Traffic_matrices = np.load('/content/drive/My Drive/TFM/RCLSTM/data/traffic-matrices.npy')\n",
        "time_step, src, dst = Traffic_matrices.shape\n",
        "traffic_1_2 = []\n",
        "for i in range(time_step):\n",
        "    traffic_1_2.append(Traffic_matrices[i][0][1])\n",
        "traffic_1_2 = np.array(traffic_1_2)\n",
        "print(traffic_1_2.shape)\n",
        "np.save('/content/drive/My Drive/TFM/RCLSTM/data/traffic-1-2.npy', traffic_1_2)\n",
        "print('traffic-1-2 has been saved')\n",
        "\n",
        "i = 0\n",
        "# Sort file names\n",
        "filenames = os.listdir(dst_path)\n",
        "new_filenames = sorted(filenames)\n",
        "\n",
        "for filename in new_filenames[1:]:\n",
        "    # Save the date and time of the file in the dictionary\n",
        "    if save_datetime:\n",
        "        date_time_dict = {}\n",
        "        portion = filename.split('-')\n",
        "        date_time_dict['year'] = portion[1]\n",
        "        date_time_dict['month'] = portion[2]\n",
        "        date_time_dict['day'] = portion[3]\n",
        "        date_time_dict['hour'] = portion[4]\n",
        "        date_time_dict['min'] = portion[5].split('.')[0]\n",
        "        date_time.append(date_time_dict)\n",
        "    # Read Traffic matrix\n",
        "    csvfile = open(os.path.join(dst_path, filename), 'rb')\n",
        "    traffic_matrix = np.loadtxt(os.path.join(dst_path, filename), delimiter=',', skiprows=0)\n",
        "    Traffic_matrices[i,:,:] = traffic_matrix\n",
        "\n",
        "    i += 1\n",
        "\n",
        "mu = np.mean(Traffic_matrices)\n",
        "std = np.std(Traffic_matrices)\n",
        "# print(mu, std)\n",
        "normalized_traffic_matrices = (Traffic_matrices - mu) / std\n",
        "\n",
        "# Save date-time as npy file\n",
        "if save_datetime:\n",
        "    np.save('/content/drive/My Drive/TFM/RCLSTM/data/date-time.npy', date_time)\n",
        "    print('date-time dicts have been saved as npy file')\n",
        "\n",
        "# Save traffic-matrices to npy file\n",
        "if save_traffic:\n",
        "    np.save('/content/drive/My Drive/TFM/RCLSTM/data/traffic-matrices.npy', Traffic_matrices)\n",
        "    print('traffic matrices has been saved as npy file')\n",
        "\n",
        "if save_normalized:\n",
        "    np.save('/content/drive/My Drive/TFM/RCLSTM/data/normalized-traffic-matrices.npy', normalized_traffic_matrices)\n",
        "    print('normalized traffic matrices has been saved as npy file')\n",
        "\n",
        "# Read traffic-matrices data\n",
        "# Traffic_matrices = np.load('/content/drive/My Drive/TFM/RCLSTM/data/traffic-matrices.npy')\n",
        "# print('=='*30)\n",
        "# normalized_traffic_matrices = np.load('/home/hyx/Pytorch/Traffic_prediction/data/normalized-traffic-matrices.npy')\n",
        "# print(normalized_traffic_matrices[0][0]*std+mu)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10772,)\n",
            "traffic-1-2 has been saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv-N1X8NLobz",
        "colab_type": "text"
      },
      "source": [
        "## Training LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZzsaBnYF0yc",
        "colab_type": "code",
        "outputId": "e408e162-acbc-45eb-a2b8-d0605d47e25b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "!pip install tensorboardcolab\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardcolab\n",
            "  Downloading https://files.pythonhosted.org/packages/d9/28/97bf50473dc058d26188ef3aae373e56173d24c615fb419705cfffa6875d/tensorboardcolab-0.0.22.tar.gz\n",
            "Building wheels for collected packages: tensorboardcolab\n",
            "  Building wheel for tensorboardcolab (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c4/aa/a0/3aaf4f1a66adbdab9b7bdd4c96db8ada89eb7cd87200cfdd32\n",
            "Successfully built tensorboardcolab\n",
            "Installing collected packages: tensorboardcolab\n",
            "Successfully installed tensorboardcolab-0.0.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqPvuZvT4RC6",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "af8f69e2-06a3-4403-f020-14dd37609790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5612
        }
      },
      "source": [
        "# encoding: utf-8\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "@author: huayuxiu\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"Train the model using traffic matrices.\"\"\"\n",
        "import argparse\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "\n",
        "!pip install tensorboardX \n",
        "from tensorboardX import SummaryWriter \n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import summary\n",
        "# %load_ext tensorboard.notebook\n",
        "\n",
        "\n",
        "np.random.seed(4)\n",
        "\n",
        "# def get_args(parser):\n",
        "#     parser.add_argument('--data', default='/content/drive/My Drive/TFM/RCLSTM/data/traffic-1-2.npy', help='path to dataset')\n",
        "#     #parser.add_argument('--model', default='lstm', choices=['lstm', 'rclstm'], help='the model to use')\n",
        "#     parser.add_argument('--connectivity', type=float, default=.5, help='the neural connectivity')\n",
        "#     parser.add_argument('--save', default='./model', help='The path to save model files')\n",
        "#     parser.add_argument('--hidden-size', type=int, default=200, help='The number of hidden units')\n",
        "#     parser.add_argument('--batch-size', type=int, default=32, help='The size of each batch')\n",
        "#     parser.add_argument('--input-size', type=int, default=1, help='The size of input data')\n",
        "#     parser.add_argument('--max-iter', type=int, default=20, help='The maximum iteration count')\n",
        "#     parser.add_argument('--gpu', default=False, action='store_true', help='The value specifying whether to use GPU')\n",
        "#     parser.add_argument('--time-window', type=int, default=100, help='The length of time window')\n",
        "#     parser.add_argument('--dropout', type=float, default=1., help='Dropout')\n",
        "#     parser.add_argument('--num-layers', type=int, default=1, help='The number of RNN layers')\n",
        "#     return parser\n",
        "\n",
        "# # Get model parameters\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser = get_args(parser)\n",
        "# args = parser.parse_args()\n",
        "# print(args)\n",
        "\n",
        "writer = SummaryWriter('/content/Graph/weights') # Saves the summaries to  the directory (weights and biases TB)\n",
        "data_path = '/content/drive/My Drive/TFM/RCLSTM/data/traffic-1-2.npy'\n",
        "#model_name = args.model\n",
        "save_dir = '/content/drive/My Drive/TFM/RCLSTM/model'\n",
        "\n",
        "#Colab form fields\n",
        "\n",
        "#@markdown Parameters \n",
        "\n",
        "use_gpu = False  #@param {type: \"boolean\"}\n",
        "tensorboard = False  #@param {type: \"boolean\"}\n",
        "max_iter= 20  #@param {type: \"slider\", min: 0, max: 50}\n",
        "hidden_size = 200 #@param {type: \"number\"}\n",
        "batch_size = 32 #@param {type: \"number\"}\n",
        "time_window = 100 #@param {type: \"number\"}\n",
        "input_size = 1 #@param {type: \"number\"}\n",
        "dropout = 0.1 #@param {type: \"number\"}\n",
        "num_layers = 1 #@param {type: \"number\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "if tensorboard == True: \n",
        "  tb = TensorBoardColab()\n",
        "\n",
        "# Disrupt data set\n",
        "def shufflelists(X, Y):\n",
        "    ri=np.random.permutation(len(X)) #vector con valores aleatorios desde 1 hasta len(X)\n",
        "    X_shuffle = [X[i].tolist() for i in ri]\n",
        "    Y_shuffle = [Y[i].tolist() for i in ri]\n",
        "    return np.array(X_shuffle), np.array(Y_shuffle)\n",
        "\n",
        "# Load data and process data\n",
        "data = np.load(data_path)  #raw_data [20232.5778 13492.2667  9087.5911 ...  4214.5333  3907.5289  3810.9689]\n",
        "\n",
        "new_data = []\n",
        "# Cargo el array con el trafico y lo normalizo con el logaritmo\n",
        "for x in data:\n",
        "    if x > 0:\n",
        "        new_data.append(np.log10(x))\n",
        "    else:\n",
        "        new_data.append(0.0000001)\n",
        "new_data = np.array(new_data) #Transform the python array into and np array\n",
        "new_data = new_data[new_data>2.5]\n",
        "data = new_data[new_data<6]\n",
        "#Con esto de arriba consigo que todos los datos esten entre 2.5 y 6\n",
        "# All data between 2.5 y 5.9\n",
        "max_data = np.max(data)\n",
        "min_data = np.min(data)\n",
        "# Applying Min-Max scaling: [0,1]\n",
        "data = (data-min_data)/(max_data-min_data)\n",
        "\n",
        "print ('Data len after processing: ', data.size) # 10759 size \n",
        "# Ahora todo esta entre 0 y 1. [0.53282104 0.48076984 0.42999951 ... 0.33129056 0.32157426 0.31835984]\n",
        "# Dataframe:  Use the pandas library to window the data and divide it into training and test sets.\n",
        "\n",
        "df = pd.DataFrame({'temp':data})\n",
        "pd.set_option('max_colwidth', 1000,'display.max_rows', 10)\n",
        "\n",
        "# define function for create N lags\n",
        "def create_lags(df, N):\n",
        "    for i in range(N):\n",
        "        df['Lag ' + str(i+1) ] = df.temp.shift(i+1)\n",
        "    return df\n",
        "\n",
        "# create time-windows lags\n",
        "df = create_lags(df,time_window)\n",
        "\n",
        "# the first 1000 days will have missing values. can't use them. (lagging)\n",
        "df = df.dropna()\n",
        "display(df)\n",
        "# create X and y (matrix)\n",
        "y = df.temp.values\n",
        "X = df.iloc[:, 1:].values\n",
        "# y --> only the first column (temp), X --> Rest of columns lags\n",
        "\n",
        "# Train on 70% of the data 10659*0.7=7461 (number of train data)\n",
        "train_idx = int(len(df) * .7)\n",
        "print('the length of train_idx: ',train_idx)\n",
        "# create train and test data\n",
        "train_X, train_Y, test_X, test_Y = X[:train_idx], y[:train_idx], X[train_idx:], y[train_idx:]\n",
        "\n",
        "print('the number of train data: ', len(train_X))\n",
        "print('the number of test data: ', len(test_X))\n",
        "print('the shape of input (train_X): ', train_X.shape)\n",
        "print('the shape of target (train_Y): ', train_Y.shape)\n",
        "\n",
        "# Calculate output and loss\n",
        "def compute_loss_accuracy(loss_fn, data, label):\n",
        "    hx = None\n",
        "    # Aqui entra al forward(self, input_, hx=None) de RNN y luego a los otros 2\n",
        "    # Le meto los batch de datos a entrenar en input_ = data\n",
        "    _, (h_n, _) = model[0](input_=data, hx=hx)\n",
        "    #print('h_n',h_n)\n",
        "    #print('h_n shape',h_n.shape) #h_n shape torch.Size([1, 32, 200])\n",
        "    logits = model[1](h_n[-1])\n",
        "    loss = torch.sqrt(loss_fn(input=logits, target=label))\n",
        "    return loss, logits\n",
        "\n",
        "#learning rate decay. 3 steps decay to one tenth per training\n",
        "def exp_lr_scheduler(optimizer, epoch, init_lr=1e-2, lr_decay_epoch=3):\n",
        "    lr = init_lr * (0.1 ** (epoch // lr_decay_epoch))\n",
        "    if epoch % lr_decay_epoch == 0:\n",
        "        print(\"LR is set to {}\".format(lr))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr \n",
        "    return optimizer , lr\n",
        "\n",
        "\n",
        "# Creates a criterion that measures the mean squared error between each element in the input x and target y.\n",
        "loss_fn = nn.MSELoss()  \n",
        "\n",
        "#Calcula el numero de batches que va a tomar dividiendo el numero de datos de entrenamiento entre el tamano de cada batch\n",
        "num_batch = int(math.ceil(len(train_X) // batch_size))\n",
        "num_batch = int(len(train_X) // batch_size)\n",
        "print('the number of batches: ', num_batch)\n",
        "\n",
        "#num_layers=1\n",
        "# In PyTorch RNN(...) automatically calls the forward function so there is no need to explicitly call RNN.forward()        \n",
        "rnn_model = RNN(input_size=input_size, hidden_size=hidden_size, \n",
        "                    num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "#Fully connected layer. Applies a linear transformation to the incoming data: y = xA^T = b\n",
        "\n",
        "fc2 = nn.Linear(in_features=hidden_size, out_features=input_size)\n",
        "\n",
        "#A sequential container. Modules will be added to it in the order they are passed in the constructor. \n",
        "#Alternatively, an ordered dict of modules can also be passed in.\n",
        "\n",
        "model = nn.Sequential(OrderedDict([('rnn', rnn_model),('fc2', fc2),]))\n",
        "\n",
        "#print(model[0])  #RNN((cell_0): LSTMCell(1, 200))\n",
        "\n",
        "#print(model[1])  #Linear(in_features=200, out_features=1, bias=True)\n",
        "\n",
        "# Moves all model parameters and buffers to the GPU\n",
        "if use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "optim_method = optim.Adam(params=model.parameters())\n",
        "iter_cnt = 0\n",
        "#max_iter = 20\n",
        "#batch_size = 32\n",
        "while iter_cnt < max_iter:\n",
        "    train_inputs, train_targets = shufflelists(train_X, train_Y)\n",
        "    optimizer , lr = exp_lr_scheduler(optim_method, iter_cnt, init_lr=0.01, lr_decay_epoch=3)\n",
        "    #lr = 0.01\n",
        "    #optimizer = optim_method\n",
        "    for i in range(num_batch):\n",
        "        # 0 32 | 32 64 | 64 96 | 96 128 y asi\n",
        "        low_index = batch_size*i\n",
        "        high_index = batch_size*(i+1)\n",
        "        # Taking batches of size 32 for the input and target data\n",
        "        if low_index <= len(train_inputs)-batch_size:\n",
        "            # Me coje trocitos de 0 a 32\n",
        "            #print('before reshape',train_inputs)\n",
        "            #print('shape before reshape',train_inputs.shape)\n",
        "            batch_inputs = train_inputs[low_index:high_index].reshape(batch_size, time_window, 1).astype(np.float32) # shape[32, 100, 1] esto son los inputs (lags) \n",
        "            # me lo pone en array 3D, Pasa de tener una matriz 2d de 7461 filas y 3 columnas a tener un array 3d con 32 matrices de 100 filas y 1 columna tipo\n",
        "            # [[[0.46492657] ...\n",
        "            #    [0.5248507 ]]\n",
        "            #    ....\n",
        "            #    [[0.38600665] ..\n",
        "            #    [0.6526436 ]]]\n",
        "            #print('after reshape',batch_inputs)\n",
        "            #print('shape after reshape',batch_inputs.shape)\n",
        "\n",
        "            batch_targets = train_targets[low_index:high_index].reshape((batch_size, 1)).astype(np.float32) # shape[32, 1] este es ahora el target que queremos conseguir \n",
        "        else:\n",
        "            batch_inputs = train_inputs[low_index:].astype(float)\n",
        "            batch_targets = train_targets[low_index:].astype(float)\n",
        "            #print('Ha entrado en el else')\n",
        "        # Create the Variable for the Autograd, converting the numpy array to Torch Tensor\n",
        "        # Ahora crea los tensores pasa de np array a tensores\n",
        "        batch_inputs = Variable(torch.from_numpy(batch_inputs), requires_grad=False)\n",
        "        batch_targets = Variable(torch.from_numpy(batch_targets), requires_grad=False)\n",
        "        # CUDA tensor types\n",
        "        if use_gpu:\n",
        "            batch_inputs = batch_inputs.cuda()\n",
        "            batch_targets = batch_targets.cuda()\n",
        "        # Sets the module in training mode\n",
        "        model.train(True)\n",
        "        \n",
        "        # Sets gradients of all model parameters to zero.\n",
        "        model.zero_grad()\n",
        "        \n",
        "        #Aqui es donde empieza a ejecutar el forward\n",
        "        train_loss, logits = compute_loss_accuracy(loss_fn=loss_fn, data=batch_inputs, label=batch_targets)\n",
        "        \n",
        "        #print('train_loss',train_loss)\n",
        "        #print('logits',logits)\n",
        "        # Runs a back-propagation operation from the loss Variable backwards through the network\n",
        "        # Scalar variables, when we call .backward() on them, don’t require arguments\n",
        "        train_loss.backward()\n",
        "        # Execute a gradient descent step based on the gradients calculated during the .backward() operation.\n",
        "        optimizer.step()\n",
        "        \n",
        "        mean_loss = []\n",
        "        # Weights, distributions and gradient scalars\n",
        "        \n",
        "        for f in model.parameters(): # model is the NN model, f is one set of parameters of the model\n",
        "            # Create a dynamic name for the histogram summary \n",
        "            # Use current parameter shape to identify the variale  \n",
        "            hist_name = 'hist' + str(list(f.grad.data.size()))\n",
        "\n",
        "            # Save the entire list of gradients of parameters f \n",
        "            writer.add_histogram(hist_name, f, i)\n",
        "\n",
        "            # Save the norm of list of gradients of parameters f \n",
        "            scalar_name = 'scalar' + str(list(f.grad.data.size()))\n",
        "            writer.add_scalar(scalar_name, torch.norm(f.grad.data).item(), i)\n",
        "            #parameter update step\n",
        "            f.data.sub_(f.grad.data * lr) \n",
        "\n",
        "            #  Tensorboard Colab Magic \n",
        "            #   with train_summary_writer.as_default():\n",
        "            #    tf.summary.scalar('loss', train_loss.data(), step=i)\n",
        "\n",
        "        if i % 20 == 0:\n",
        "            print('the %dth iter, the %dth batch, train loss is %.4f' % (iter_cnt, i, train_loss.data))\n",
        "            mean_loss.append(train_loss.data)  \n",
        "            #  Tensorboad Colab Library          \n",
        "            if tensorboard == True: \n",
        "                tb.save_value('Train Loss', 'train_loss', i, train_loss.data)\n",
        "        \n",
        "    # Save the model once per iteration\n",
        "            save_path = '{}/'.format(save_dir)\n",
        "    if os.path.exists(save_path):\n",
        "        torch.save(model, os.path.join(save_path, str(iter_cnt)+'.pt'))\n",
        "    else:\n",
        "        os.makedirs(save_path)\n",
        "        torch.save(model, os.path.join(save_path, str(iter_cnt)+'.pt'))\n",
        "    iter_cnt += 1\n",
        "    \n",
        "    \n",
        "# Print the loss \n",
        "\n",
        "plt.figure(figsize=(16, 7))\n",
        "plt.title('Training loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.plot(mean_loss)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (1.6)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (41.0.1)\n",
            "Data len after processing:  10759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>temp</th>\n",
              "      <th>Lag 1</th>\n",
              "      <th>Lag 2</th>\n",
              "      <th>Lag 3</th>\n",
              "      <th>Lag 4</th>\n",
              "      <th>Lag 5</th>\n",
              "      <th>Lag 6</th>\n",
              "      <th>Lag 7</th>\n",
              "      <th>Lag 8</th>\n",
              "      <th>Lag 9</th>\n",
              "      <th>...</th>\n",
              "      <th>Lag 91</th>\n",
              "      <th>Lag 92</th>\n",
              "      <th>Lag 93</th>\n",
              "      <th>Lag 94</th>\n",
              "      <th>Lag 95</th>\n",
              "      <th>Lag 96</th>\n",
              "      <th>Lag 97</th>\n",
              "      <th>Lag 98</th>\n",
              "      <th>Lag 99</th>\n",
              "      <th>Lag 100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.526936</td>\n",
              "      <td>0.532788</td>\n",
              "      <td>0.547023</td>\n",
              "      <td>0.501063</td>\n",
              "      <td>0.513839</td>\n",
              "      <td>0.508366</td>\n",
              "      <td>0.521265</td>\n",
              "      <td>0.492559</td>\n",
              "      <td>0.494409</td>\n",
              "      <td>0.506479</td>\n",
              "      <td>...</td>\n",
              "      <td>0.477021</td>\n",
              "      <td>0.352035</td>\n",
              "      <td>0.422471</td>\n",
              "      <td>0.436998</td>\n",
              "      <td>0.479496</td>\n",
              "      <td>0.449577</td>\n",
              "      <td>0.416339</td>\n",
              "      <td>0.430000</td>\n",
              "      <td>0.480770</td>\n",
              "      <td>0.532821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.540273</td>\n",
              "      <td>0.526936</td>\n",
              "      <td>0.532788</td>\n",
              "      <td>0.547023</td>\n",
              "      <td>0.501063</td>\n",
              "      <td>0.513839</td>\n",
              "      <td>0.508366</td>\n",
              "      <td>0.521265</td>\n",
              "      <td>0.492559</td>\n",
              "      <td>0.494409</td>\n",
              "      <td>...</td>\n",
              "      <td>0.539258</td>\n",
              "      <td>0.477021</td>\n",
              "      <td>0.352035</td>\n",
              "      <td>0.422471</td>\n",
              "      <td>0.436998</td>\n",
              "      <td>0.479496</td>\n",
              "      <td>0.449577</td>\n",
              "      <td>0.416339</td>\n",
              "      <td>0.430000</td>\n",
              "      <td>0.480770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.500653</td>\n",
              "      <td>0.540273</td>\n",
              "      <td>0.526936</td>\n",
              "      <td>0.532788</td>\n",
              "      <td>0.547023</td>\n",
              "      <td>0.501063</td>\n",
              "      <td>0.513839</td>\n",
              "      <td>0.508366</td>\n",
              "      <td>0.521265</td>\n",
              "      <td>0.492559</td>\n",
              "      <td>...</td>\n",
              "      <td>0.524269</td>\n",
              "      <td>0.539258</td>\n",
              "      <td>0.477021</td>\n",
              "      <td>0.352035</td>\n",
              "      <td>0.422471</td>\n",
              "      <td>0.436998</td>\n",
              "      <td>0.479496</td>\n",
              "      <td>0.449577</td>\n",
              "      <td>0.416339</td>\n",
              "      <td>0.430000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.492643</td>\n",
              "      <td>0.500653</td>\n",
              "      <td>0.540273</td>\n",
              "      <td>0.526936</td>\n",
              "      <td>0.532788</td>\n",
              "      <td>0.547023</td>\n",
              "      <td>0.501063</td>\n",
              "      <td>0.513839</td>\n",
              "      <td>0.508366</td>\n",
              "      <td>0.521265</td>\n",
              "      <td>...</td>\n",
              "      <td>0.518840</td>\n",
              "      <td>0.524269</td>\n",
              "      <td>0.539258</td>\n",
              "      <td>0.477021</td>\n",
              "      <td>0.352035</td>\n",
              "      <td>0.422471</td>\n",
              "      <td>0.436998</td>\n",
              "      <td>0.479496</td>\n",
              "      <td>0.449577</td>\n",
              "      <td>0.416339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>0.516756</td>\n",
              "      <td>0.492643</td>\n",
              "      <td>0.500653</td>\n",
              "      <td>0.540273</td>\n",
              "      <td>0.526936</td>\n",
              "      <td>0.532788</td>\n",
              "      <td>0.547023</td>\n",
              "      <td>0.501063</td>\n",
              "      <td>0.513839</td>\n",
              "      <td>0.508366</td>\n",
              "      <td>...</td>\n",
              "      <td>0.513700</td>\n",
              "      <td>0.518840</td>\n",
              "      <td>0.524269</td>\n",
              "      <td>0.539258</td>\n",
              "      <td>0.477021</td>\n",
              "      <td>0.352035</td>\n",
              "      <td>0.422471</td>\n",
              "      <td>0.436998</td>\n",
              "      <td>0.479496</td>\n",
              "      <td>0.449577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10754</th>\n",
              "      <td>0.237266</td>\n",
              "      <td>0.347280</td>\n",
              "      <td>0.290197</td>\n",
              "      <td>0.331173</td>\n",
              "      <td>0.359224</td>\n",
              "      <td>0.260285</td>\n",
              "      <td>0.423201</td>\n",
              "      <td>0.378452</td>\n",
              "      <td>0.249774</td>\n",
              "      <td>0.253864</td>\n",
              "      <td>...</td>\n",
              "      <td>0.508270</td>\n",
              "      <td>0.480566</td>\n",
              "      <td>0.505662</td>\n",
              "      <td>0.518625</td>\n",
              "      <td>0.444280</td>\n",
              "      <td>0.476197</td>\n",
              "      <td>0.448437</td>\n",
              "      <td>0.521507</td>\n",
              "      <td>0.578357</td>\n",
              "      <td>0.474246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10755</th>\n",
              "      <td>0.318283</td>\n",
              "      <td>0.237266</td>\n",
              "      <td>0.347280</td>\n",
              "      <td>0.290197</td>\n",
              "      <td>0.331173</td>\n",
              "      <td>0.359224</td>\n",
              "      <td>0.260285</td>\n",
              "      <td>0.423201</td>\n",
              "      <td>0.378452</td>\n",
              "      <td>0.249774</td>\n",
              "      <td>...</td>\n",
              "      <td>0.489696</td>\n",
              "      <td>0.508270</td>\n",
              "      <td>0.480566</td>\n",
              "      <td>0.505662</td>\n",
              "      <td>0.518625</td>\n",
              "      <td>0.444280</td>\n",
              "      <td>0.476197</td>\n",
              "      <td>0.448437</td>\n",
              "      <td>0.521507</td>\n",
              "      <td>0.578357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10756</th>\n",
              "      <td>0.331291</td>\n",
              "      <td>0.318283</td>\n",
              "      <td>0.237266</td>\n",
              "      <td>0.347280</td>\n",
              "      <td>0.290197</td>\n",
              "      <td>0.331173</td>\n",
              "      <td>0.359224</td>\n",
              "      <td>0.260285</td>\n",
              "      <td>0.423201</td>\n",
              "      <td>0.378452</td>\n",
              "      <td>...</td>\n",
              "      <td>0.416738</td>\n",
              "      <td>0.489696</td>\n",
              "      <td>0.508270</td>\n",
              "      <td>0.480566</td>\n",
              "      <td>0.505662</td>\n",
              "      <td>0.518625</td>\n",
              "      <td>0.444280</td>\n",
              "      <td>0.476197</td>\n",
              "      <td>0.448437</td>\n",
              "      <td>0.521507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10757</th>\n",
              "      <td>0.321574</td>\n",
              "      <td>0.331291</td>\n",
              "      <td>0.318283</td>\n",
              "      <td>0.237266</td>\n",
              "      <td>0.347280</td>\n",
              "      <td>0.290197</td>\n",
              "      <td>0.331173</td>\n",
              "      <td>0.359224</td>\n",
              "      <td>0.260285</td>\n",
              "      <td>0.423201</td>\n",
              "      <td>...</td>\n",
              "      <td>0.351544</td>\n",
              "      <td>0.416738</td>\n",
              "      <td>0.489696</td>\n",
              "      <td>0.508270</td>\n",
              "      <td>0.480566</td>\n",
              "      <td>0.505662</td>\n",
              "      <td>0.518625</td>\n",
              "      <td>0.444280</td>\n",
              "      <td>0.476197</td>\n",
              "      <td>0.448437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10758</th>\n",
              "      <td>0.318360</td>\n",
              "      <td>0.321574</td>\n",
              "      <td>0.331291</td>\n",
              "      <td>0.318283</td>\n",
              "      <td>0.237266</td>\n",
              "      <td>0.347280</td>\n",
              "      <td>0.290197</td>\n",
              "      <td>0.331173</td>\n",
              "      <td>0.359224</td>\n",
              "      <td>0.260285</td>\n",
              "      <td>...</td>\n",
              "      <td>0.364335</td>\n",
              "      <td>0.351544</td>\n",
              "      <td>0.416738</td>\n",
              "      <td>0.489696</td>\n",
              "      <td>0.508270</td>\n",
              "      <td>0.480566</td>\n",
              "      <td>0.505662</td>\n",
              "      <td>0.518625</td>\n",
              "      <td>0.444280</td>\n",
              "      <td>0.476197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10659 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           temp     Lag 1     Lag 2     Lag 3     Lag 4     Lag 5     Lag 6  \\\n",
              "100    0.526936  0.532788  0.547023  0.501063  0.513839  0.508366  0.521265   \n",
              "101    0.540273  0.526936  0.532788  0.547023  0.501063  0.513839  0.508366   \n",
              "102    0.500653  0.540273  0.526936  0.532788  0.547023  0.501063  0.513839   \n",
              "103    0.492643  0.500653  0.540273  0.526936  0.532788  0.547023  0.501063   \n",
              "104    0.516756  0.492643  0.500653  0.540273  0.526936  0.532788  0.547023   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "10754  0.237266  0.347280  0.290197  0.331173  0.359224  0.260285  0.423201   \n",
              "10755  0.318283  0.237266  0.347280  0.290197  0.331173  0.359224  0.260285   \n",
              "10756  0.331291  0.318283  0.237266  0.347280  0.290197  0.331173  0.359224   \n",
              "10757  0.321574  0.331291  0.318283  0.237266  0.347280  0.290197  0.331173   \n",
              "10758  0.318360  0.321574  0.331291  0.318283  0.237266  0.347280  0.290197   \n",
              "\n",
              "          Lag 7     Lag 8     Lag 9  ...    Lag 91    Lag 92    Lag 93  \\\n",
              "100    0.492559  0.494409  0.506479  ...  0.477021  0.352035  0.422471   \n",
              "101    0.521265  0.492559  0.494409  ...  0.539258  0.477021  0.352035   \n",
              "102    0.508366  0.521265  0.492559  ...  0.524269  0.539258  0.477021   \n",
              "103    0.513839  0.508366  0.521265  ...  0.518840  0.524269  0.539258   \n",
              "104    0.501063  0.513839  0.508366  ...  0.513700  0.518840  0.524269   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "10754  0.378452  0.249774  0.253864  ...  0.508270  0.480566  0.505662   \n",
              "10755  0.423201  0.378452  0.249774  ...  0.489696  0.508270  0.480566   \n",
              "10756  0.260285  0.423201  0.378452  ...  0.416738  0.489696  0.508270   \n",
              "10757  0.359224  0.260285  0.423201  ...  0.351544  0.416738  0.489696   \n",
              "10758  0.331173  0.359224  0.260285  ...  0.364335  0.351544  0.416738   \n",
              "\n",
              "         Lag 94    Lag 95    Lag 96    Lag 97    Lag 98    Lag 99   Lag 100  \n",
              "100    0.436998  0.479496  0.449577  0.416339  0.430000  0.480770  0.532821  \n",
              "101    0.422471  0.436998  0.479496  0.449577  0.416339  0.430000  0.480770  \n",
              "102    0.352035  0.422471  0.436998  0.479496  0.449577  0.416339  0.430000  \n",
              "103    0.477021  0.352035  0.422471  0.436998  0.479496  0.449577  0.416339  \n",
              "104    0.539258  0.477021  0.352035  0.422471  0.436998  0.479496  0.449577  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "10754  0.518625  0.444280  0.476197  0.448437  0.521507  0.578357  0.474246  \n",
              "10755  0.505662  0.518625  0.444280  0.476197  0.448437  0.521507  0.578357  \n",
              "10756  0.480566  0.505662  0.518625  0.444280  0.476197  0.448437  0.521507  \n",
              "10757  0.508270  0.480566  0.505662  0.518625  0.444280  0.476197  0.448437  \n",
              "10758  0.489696  0.508270  0.480566  0.505662  0.518625  0.444280  0.476197  \n",
              "\n",
              "[10659 rows x 101 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "the length of train_idx:  7461\n",
            "the number of train data:  7461\n",
            "the number of test data:  3198\n",
            "the shape of input (train_X):  (7461, 100)\n",
            "the shape of target (train_Y):  (7461,)\n",
            "the number of batches:  233\n",
            "LR is set to 0.01\n",
            "the 0th iter, the 0th batch, train loss is 0.4722\n",
            "the 0th iter, the 20th batch, train loss is 0.1678\n",
            "the 0th iter, the 40th batch, train loss is 0.1160\n",
            "the 0th iter, the 60th batch, train loss is 0.1108\n",
            "the 0th iter, the 80th batch, train loss is 0.1129\n",
            "the 0th iter, the 100th batch, train loss is 0.0875\n",
            "the 0th iter, the 120th batch, train loss is 0.0883\n",
            "the 0th iter, the 140th batch, train loss is 0.0958\n",
            "the 0th iter, the 160th batch, train loss is 0.0735\n",
            "the 0th iter, the 180th batch, train loss is 0.0775\n",
            "the 0th iter, the 200th batch, train loss is 0.0925\n",
            "the 0th iter, the 220th batch, train loss is 0.0617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LSTMCell. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "the 1th iter, the 0th batch, train loss is 0.0824\n",
            "the 1th iter, the 20th batch, train loss is 0.0788\n",
            "the 1th iter, the 40th batch, train loss is 0.0741\n",
            "the 1th iter, the 60th batch, train loss is 0.0688\n",
            "the 1th iter, the 80th batch, train loss is 0.0646\n",
            "the 1th iter, the 100th batch, train loss is 0.0815\n",
            "the 1th iter, the 120th batch, train loss is 0.1037\n",
            "the 1th iter, the 140th batch, train loss is 0.0580\n",
            "the 1th iter, the 160th batch, train loss is 0.0861\n",
            "the 1th iter, the 180th batch, train loss is 0.0663\n",
            "the 1th iter, the 200th batch, train loss is 0.0799\n",
            "the 1th iter, the 220th batch, train loss is 0.0767\n",
            "the 2th iter, the 0th batch, train loss is 0.0682\n",
            "the 2th iter, the 20th batch, train loss is 0.0921\n",
            "the 2th iter, the 40th batch, train loss is 0.0795\n",
            "the 2th iter, the 60th batch, train loss is 0.1173\n",
            "the 2th iter, the 80th batch, train loss is 0.0667\n",
            "the 2th iter, the 100th batch, train loss is 0.0867\n",
            "the 2th iter, the 120th batch, train loss is 0.0982\n",
            "the 2th iter, the 140th batch, train loss is 0.0650\n",
            "the 2th iter, the 160th batch, train loss is 0.0819\n",
            "the 2th iter, the 180th batch, train loss is 0.0962\n",
            "the 2th iter, the 200th batch, train loss is 0.0958\n",
            "the 2th iter, the 220th batch, train loss is 0.0738\n",
            "LR is set to 0.001\n",
            "the 3th iter, the 0th batch, train loss is 0.0519\n",
            "the 3th iter, the 20th batch, train loss is 0.0768\n",
            "the 3th iter, the 40th batch, train loss is 0.0950\n",
            "the 3th iter, the 60th batch, train loss is 0.0716\n",
            "the 3th iter, the 80th batch, train loss is 0.0719\n",
            "the 3th iter, the 100th batch, train loss is 0.0835\n",
            "the 3th iter, the 120th batch, train loss is 0.0822\n",
            "the 3th iter, the 140th batch, train loss is 0.1012\n",
            "the 3th iter, the 160th batch, train loss is 0.0966\n",
            "the 3th iter, the 180th batch, train loss is 0.0667\n",
            "the 3th iter, the 200th batch, train loss is 0.0757\n",
            "the 3th iter, the 220th batch, train loss is 0.1104\n",
            "the 4th iter, the 0th batch, train loss is 0.0721\n",
            "the 4th iter, the 20th batch, train loss is 0.0666\n",
            "the 4th iter, the 40th batch, train loss is 0.0891\n",
            "the 4th iter, the 60th batch, train loss is 0.0828\n",
            "the 4th iter, the 80th batch, train loss is 0.1115\n",
            "the 4th iter, the 100th batch, train loss is 0.0855\n",
            "the 4th iter, the 120th batch, train loss is 0.0637\n",
            "the 4th iter, the 140th batch, train loss is 0.0823\n",
            "the 4th iter, the 160th batch, train loss is 0.0801\n",
            "the 4th iter, the 180th batch, train loss is 0.1236\n",
            "the 4th iter, the 200th batch, train loss is 0.0930\n",
            "the 4th iter, the 220th batch, train loss is 0.0661\n",
            "the 5th iter, the 0th batch, train loss is 0.0763\n",
            "the 5th iter, the 20th batch, train loss is 0.0712\n",
            "the 5th iter, the 40th batch, train loss is 0.1383\n",
            "the 5th iter, the 60th batch, train loss is 0.0729\n",
            "the 5th iter, the 80th batch, train loss is 0.0769\n",
            "the 5th iter, the 100th batch, train loss is 0.0638\n",
            "the 5th iter, the 120th batch, train loss is 0.0855\n",
            "the 5th iter, the 140th batch, train loss is 0.0669\n",
            "the 5th iter, the 160th batch, train loss is 0.0586\n",
            "the 5th iter, the 180th batch, train loss is 0.0733\n",
            "the 5th iter, the 200th batch, train loss is 0.0749\n",
            "the 5th iter, the 220th batch, train loss is 0.0663\n",
            "LR is set to 0.00010000000000000002\n",
            "the 6th iter, the 0th batch, train loss is 0.0930\n",
            "the 6th iter, the 20th batch, train loss is 0.0766\n",
            "the 6th iter, the 40th batch, train loss is 0.0837\n",
            "the 6th iter, the 60th batch, train loss is 0.0830\n",
            "the 6th iter, the 80th batch, train loss is 0.0651\n",
            "the 6th iter, the 100th batch, train loss is 0.1159\n",
            "the 6th iter, the 120th batch, train loss is 0.0671\n",
            "the 6th iter, the 140th batch, train loss is 0.0618\n",
            "the 6th iter, the 160th batch, train loss is 0.0992\n",
            "the 6th iter, the 180th batch, train loss is 0.0911\n",
            "the 6th iter, the 200th batch, train loss is 0.1024\n",
            "the 6th iter, the 220th batch, train loss is 0.0573\n",
            "the 7th iter, the 0th batch, train loss is 0.0674\n",
            "the 7th iter, the 20th batch, train loss is 0.0770\n",
            "the 7th iter, the 40th batch, train loss is 0.0833\n",
            "the 7th iter, the 60th batch, train loss is 0.0781\n",
            "the 7th iter, the 80th batch, train loss is 0.0854\n",
            "the 7th iter, the 100th batch, train loss is 0.0783\n",
            "the 7th iter, the 120th batch, train loss is 0.0581\n",
            "the 7th iter, the 140th batch, train loss is 0.0660\n",
            "the 7th iter, the 160th batch, train loss is 0.0787\n",
            "the 7th iter, the 180th batch, train loss is 0.1286\n",
            "the 7th iter, the 200th batch, train loss is 0.0965\n",
            "the 7th iter, the 220th batch, train loss is 0.0823\n",
            "the 8th iter, the 0th batch, train loss is 0.0727\n",
            "the 8th iter, the 20th batch, train loss is 0.0855\n",
            "the 8th iter, the 40th batch, train loss is 0.0846\n",
            "the 8th iter, the 60th batch, train loss is 0.0701\n",
            "the 8th iter, the 80th batch, train loss is 0.0717\n",
            "the 8th iter, the 100th batch, train loss is 0.0633\n",
            "the 8th iter, the 120th batch, train loss is 0.0628\n",
            "the 8th iter, the 140th batch, train loss is 0.1132\n",
            "the 8th iter, the 160th batch, train loss is 0.0628\n",
            "the 8th iter, the 180th batch, train loss is 0.0780\n",
            "the 8th iter, the 200th batch, train loss is 0.0667\n",
            "the 8th iter, the 220th batch, train loss is 0.0756\n",
            "LR is set to 1.0000000000000003e-05\n",
            "the 9th iter, the 0th batch, train loss is 0.1063\n",
            "the 9th iter, the 20th batch, train loss is 0.0833\n",
            "the 9th iter, the 40th batch, train loss is 0.0680\n",
            "the 9th iter, the 60th batch, train loss is 0.0724\n",
            "the 9th iter, the 80th batch, train loss is 0.0597\n",
            "the 9th iter, the 100th batch, train loss is 0.0791\n",
            "the 9th iter, the 120th batch, train loss is 0.0766\n",
            "the 9th iter, the 140th batch, train loss is 0.0930\n",
            "the 9th iter, the 160th batch, train loss is 0.0914\n",
            "the 9th iter, the 180th batch, train loss is 0.0697\n",
            "the 9th iter, the 200th batch, train loss is 0.0601\n",
            "the 9th iter, the 220th batch, train loss is 0.0970\n",
            "the 10th iter, the 0th batch, train loss is 0.0884\n",
            "the 10th iter, the 20th batch, train loss is 0.0604\n",
            "the 10th iter, the 40th batch, train loss is 0.1283\n",
            "the 10th iter, the 60th batch, train loss is 0.0888\n",
            "the 10th iter, the 80th batch, train loss is 0.0853\n",
            "the 10th iter, the 100th batch, train loss is 0.0897\n",
            "the 10th iter, the 120th batch, train loss is 0.1103\n",
            "the 10th iter, the 140th batch, train loss is 0.0713\n",
            "the 10th iter, the 160th batch, train loss is 0.0668\n",
            "the 10th iter, the 180th batch, train loss is 0.0855\n",
            "the 10th iter, the 200th batch, train loss is 0.0551\n",
            "the 10th iter, the 220th batch, train loss is 0.0874\n",
            "the 11th iter, the 0th batch, train loss is 0.1043\n",
            "the 11th iter, the 20th batch, train loss is 0.0760\n",
            "the 11th iter, the 40th batch, train loss is 0.0638\n",
            "the 11th iter, the 60th batch, train loss is 0.0798\n",
            "the 11th iter, the 80th batch, train loss is 0.0788\n",
            "the 11th iter, the 100th batch, train loss is 0.0406\n",
            "the 11th iter, the 120th batch, train loss is 0.0945\n",
            "the 11th iter, the 140th batch, train loss is 0.0680\n",
            "the 11th iter, the 160th batch, train loss is 0.0759\n",
            "the 11th iter, the 180th batch, train loss is 0.1003\n",
            "the 11th iter, the 200th batch, train loss is 0.0604\n",
            "the 11th iter, the 220th batch, train loss is 0.0754\n",
            "LR is set to 1.0000000000000002e-06\n",
            "the 12th iter, the 0th batch, train loss is 0.0892\n",
            "the 12th iter, the 20th batch, train loss is 0.0799\n",
            "the 12th iter, the 40th batch, train loss is 0.0666\n",
            "the 12th iter, the 60th batch, train loss is 0.0687\n",
            "the 12th iter, the 80th batch, train loss is 0.0721\n",
            "the 12th iter, the 100th batch, train loss is 0.0775\n",
            "the 12th iter, the 120th batch, train loss is 0.0560\n",
            "the 12th iter, the 140th batch, train loss is 0.0637\n",
            "the 12th iter, the 160th batch, train loss is 0.0560\n",
            "the 12th iter, the 180th batch, train loss is 0.0713\n",
            "the 12th iter, the 200th batch, train loss is 0.0854\n",
            "the 12th iter, the 220th batch, train loss is 0.0908\n",
            "the 13th iter, the 0th batch, train loss is 0.0588\n",
            "the 13th iter, the 20th batch, train loss is 0.0594\n",
            "the 13th iter, the 40th batch, train loss is 0.0740\n",
            "the 13th iter, the 60th batch, train loss is 0.0642\n",
            "the 13th iter, the 80th batch, train loss is 0.0728\n",
            "the 13th iter, the 100th batch, train loss is 0.0713\n",
            "the 13th iter, the 120th batch, train loss is 0.0988\n",
            "the 13th iter, the 140th batch, train loss is 0.0695\n",
            "the 13th iter, the 160th batch, train loss is 0.0766\n",
            "the 13th iter, the 180th batch, train loss is 0.0677\n",
            "the 13th iter, the 200th batch, train loss is 0.0851\n",
            "the 13th iter, the 220th batch, train loss is 0.0844\n",
            "the 14th iter, the 0th batch, train loss is 0.0802\n",
            "the 14th iter, the 20th batch, train loss is 0.1121\n",
            "the 14th iter, the 40th batch, train loss is 0.0803\n",
            "the 14th iter, the 60th batch, train loss is 0.0510\n",
            "the 14th iter, the 80th batch, train loss is 0.0792\n",
            "the 14th iter, the 100th batch, train loss is 0.0589\n",
            "the 14th iter, the 120th batch, train loss is 0.0839\n",
            "the 14th iter, the 140th batch, train loss is 0.0880\n",
            "the 14th iter, the 160th batch, train loss is 0.0758\n",
            "the 14th iter, the 180th batch, train loss is 0.0622\n",
            "the 14th iter, the 200th batch, train loss is 0.0840\n",
            "the 14th iter, the 220th batch, train loss is 0.0687\n",
            "LR is set to 1.0000000000000002e-07\n",
            "the 15th iter, the 0th batch, train loss is 0.0735\n",
            "the 15th iter, the 20th batch, train loss is 0.0685\n",
            "the 15th iter, the 40th batch, train loss is 0.0869\n",
            "the 15th iter, the 60th batch, train loss is 0.0725\n",
            "the 15th iter, the 80th batch, train loss is 0.0648\n",
            "the 15th iter, the 100th batch, train loss is 0.0653\n",
            "the 15th iter, the 120th batch, train loss is 0.0703\n",
            "the 15th iter, the 140th batch, train loss is 0.0759\n",
            "the 15th iter, the 160th batch, train loss is 0.0779\n",
            "the 15th iter, the 180th batch, train loss is 0.1122\n",
            "the 15th iter, the 200th batch, train loss is 0.0864\n",
            "the 15th iter, the 220th batch, train loss is 0.1150\n",
            "the 16th iter, the 0th batch, train loss is 0.0597\n",
            "the 16th iter, the 20th batch, train loss is 0.0730\n",
            "the 16th iter, the 40th batch, train loss is 0.0684\n",
            "the 16th iter, the 60th batch, train loss is 0.0656\n",
            "the 16th iter, the 80th batch, train loss is 0.0807\n",
            "the 16th iter, the 100th batch, train loss is 0.0799\n",
            "the 16th iter, the 120th batch, train loss is 0.0848\n",
            "the 16th iter, the 140th batch, train loss is 0.0801\n",
            "the 16th iter, the 160th batch, train loss is 0.1222\n",
            "the 16th iter, the 180th batch, train loss is 0.0819\n",
            "the 16th iter, the 200th batch, train loss is 0.0747\n",
            "the 16th iter, the 220th batch, train loss is 0.0935\n",
            "the 17th iter, the 0th batch, train loss is 0.0808\n",
            "the 17th iter, the 20th batch, train loss is 0.0607\n",
            "the 17th iter, the 40th batch, train loss is 0.0872\n",
            "the 17th iter, the 60th batch, train loss is 0.0704\n",
            "the 17th iter, the 80th batch, train loss is 0.1365\n",
            "the 17th iter, the 100th batch, train loss is 0.0636\n",
            "the 17th iter, the 120th batch, train loss is 0.1010\n",
            "the 17th iter, the 140th batch, train loss is 0.1007\n",
            "the 17th iter, the 160th batch, train loss is 0.0899\n",
            "the 17th iter, the 180th batch, train loss is 0.0870\n",
            "the 17th iter, the 200th batch, train loss is 0.0632\n",
            "the 17th iter, the 220th batch, train loss is 0.0859\n",
            "LR is set to 1.0000000000000004e-08\n",
            "the 18th iter, the 0th batch, train loss is 0.0605\n",
            "the 18th iter, the 20th batch, train loss is 0.0886\n",
            "the 18th iter, the 40th batch, train loss is 0.1259\n",
            "the 18th iter, the 60th batch, train loss is 0.0846\n",
            "the 18th iter, the 80th batch, train loss is 0.0815\n",
            "the 18th iter, the 100th batch, train loss is 0.0513\n",
            "the 18th iter, the 120th batch, train loss is 0.0766\n",
            "the 18th iter, the 140th batch, train loss is 0.0792\n",
            "the 18th iter, the 160th batch, train loss is 0.0967\n",
            "the 18th iter, the 180th batch, train loss is 0.0597\n",
            "the 18th iter, the 200th batch, train loss is 0.1037\n",
            "the 18th iter, the 220th batch, train loss is 0.0578\n",
            "the 19th iter, the 0th batch, train loss is 0.0885\n",
            "the 19th iter, the 20th batch, train loss is 0.1014\n",
            "the 19th iter, the 40th batch, train loss is 0.0590\n",
            "the 19th iter, the 60th batch, train loss is 0.0854\n",
            "the 19th iter, the 80th batch, train loss is 0.0765\n",
            "the 19th iter, the 100th batch, train loss is 0.0670\n",
            "the 19th iter, the 120th batch, train loss is 0.0590\n",
            "the 19th iter, the 140th batch, train loss is 0.0805\n",
            "the 19th iter, the 160th batch, train loss is 0.0737\n",
            "the 19th iter, the 180th batch, train loss is 0.0492\n",
            "the 19th iter, the 200th batch, train loss is 0.0742\n",
            "the 19th iter, the 220th batch, train loss is 0.0996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8IAAAG5CAYAAABFkY2GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGIoQBgAAYChCGAAAgKEIYQAAAIby/wBg4L2d4zudPQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1152x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04trd_NRUSoa",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM Cell / RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJshTby-URsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoding: utf-8\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "@author: huayux\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional, init\n",
        "import numpy as np\n",
        "#from util import logFunction\n",
        "import random\n",
        "import math\n",
        "\n",
        "\n",
        "# Building a basic LSTM unit\n",
        "class LSTMCell(nn.Module):\n",
        "\n",
        "    \"\"\"A basic LSTM cell.\"\"\"\n",
        "# Parameters (input_size,hidden_size,bias) If false, the layer doesnt use bias weights b_ih/b_hh Default: true.\n",
        "# __init__ represent the constructor in Python.\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        \"\"\"\n",
        "        Most parts are copied from torch.nn.LSTMCell.\n",
        "        \"\"\"\n",
        "# weight_ih: the learnable input-hidden weights, of shape (4*hidden_size x input_size)\n",
        "# weight_hh: the learnable hidden-hidden weights, of shape (4*hidden_size x hidden_size)\n",
        "        super(LSTMCell, self).__init__()\n",
        "        #self.op = logFunction()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.weight_ih = nn.Parameter(torch.FloatTensor(input_size, 4 * hidden_size))\n",
        "        self.weight_hh = nn.Parameter(torch.FloatTensor(hidden_size, 4 * hidden_size))\n",
        "        self.bias = nn.Parameter(torch.FloatTensor(4 * hidden_size))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Initialize parameters following the way proposed in the paper.\n",
        "        \"\"\"\n",
        "        init.xavier_uniform_(self.weight_ih.data, gain=init.calculate_gain('sigmoid'))\n",
        "        init.xavier_uniform_(self.weight_hh.data, gain=init.calculate_gain('sigmoid'))\n",
        "        init.constant_(self.bias.data, val=0)\n",
        "\n",
        "    def forward(self, input_, hx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_: A (batch, input_size) tensor containing input\n",
        "                features.\n",
        "            hx: A tuple (h_0, c_0), which contains the initial hidden\n",
        "                and cell state, where the size of both states is\n",
        "                (batch, hidden_size).\n",
        "\n",
        "        Returns:\n",
        "            h_1, c_1: Tensors containing the next hidden and cell state.\n",
        "        \"\"\"\n",
        "        #print('entra en el forward de RNN 1')\n",
        "        h_0, c_0 = hx\n",
        "        batch_size = h_0.size(0)\n",
        "        bias_batch = (self.bias.unsqueeze(0).expand(batch_size, *self.bias.size()))\n",
        "        wh_b = torch.addmm(bias_batch, h_0, self.weight_hh)\n",
        "        wi = torch.mm(input_, self.weight_ih)\n",
        "        f, i, o, g = torch.split(wh_b + wi, self.hidden_size, dim=1)\n",
        "        c_1 = torch.sigmoid(f)*c_0 + torch.sigmoid(i)*torch.tanh(g)\n",
        "        h_1 = torch.sigmoid(o) * torch.tanh(c_1)\n",
        "        return h_1, c_1\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = '{name}({input_size}, {hidden_size})'\n",
        "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
        "\n",
        "\n",
        "\n",
        "# Construct the RNN network. If cell_class is lstm, then the hidden neurons of the network are LSTM units;\n",
        "# If cell_class is rclstm, then the hidden neuron of the network is rclstm\n",
        "# The num_layer parameter indicates the number of network layers.\n",
        "class RNN(nn.Module):\n",
        "\n",
        "    \"\"\"A module that runs multiple steps of LSTM or RCLSTM.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, batch_first=True, dropout=0):\n",
        "    \n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.dropout = dropout\n",
        "\n",
        "        for layer in range(num_layers):\n",
        "            layer_input_size = input_size if layer == 0 else hidden_size\n",
        "            \n",
        "            cell = LSTMCell(input_size=layer_input_size, hidden_size=hidden_size)\n",
        "            \n",
        "            setattr(self, 'cell_{}'.format(layer), cell)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def get_cell(self, layer):\n",
        "        return getattr(self, 'cell_{}'.format(layer))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in range(self.num_layers):\n",
        "            cell = self.get_cell(layer) # LSTMCell(1, 200)\n",
        "            \n",
        "\n",
        "    @staticmethod\n",
        "    def _forward_rnn(cell, input_, hx):\n",
        "        max_time = input_.size(0)\n",
        "        #print(max_time)\n",
        "        output = []\n",
        "        for time in range(max_time):\n",
        "            # Aqui entra en el primer forward\n",
        "            h_next, c_next = cell(input_=input_[max_time-1-time], hx=hx)\n",
        "            \n",
        "            hx_next = (h_next, c_next)\n",
        "            output.append(h_next)\n",
        "            hx = hx_next\n",
        "        output = torch.stack(output, 0)\n",
        "        \n",
        "        return output, hx\n",
        "\n",
        "    def forward(self, input_, hx=None):\n",
        "        if self.batch_first:\n",
        "            #bath con los datos de entrada\n",
        "            input_ = input_.transpose(0, 1)\n",
        "        # (100,32,1) esto es lo que ha permutado antes era (32,100,1)    ahora tengo 100 matrices de 32 filas y 1 columna\n",
        "        max_time, batch_size, _ = input_.size()\n",
        "        #print(max_time)\n",
        "        #print(batch_size)\n",
        "        if hx is None:\n",
        "            # hx es un tensor con zeros con las dimensiones que se le pasam\n",
        "            hx = Variable(input_.data.new(batch_size, self.hidden_size).zero_())\n",
        "            # torch.Size([32, 200])\n",
        "            hx = (hx, hx)\n",
        "           \n",
        "        h_n = []\n",
        "        c_n = []\n",
        "        layer_output = None\n",
        "        for layer in range(self.num_layers):\n",
        "            cell = self.get_cell(layer)\n",
        "            #Aqui entra en el segundo forward\n",
        "            layer_output, (layer_h_n, layer_c_n) = RNN._forward_rnn(\n",
        "                cell=cell, input_=input_, hx=hx)\n",
        "            input_ = layer_output\n",
        "            h_n.append(layer_h_n)\n",
        "            c_n.append(layer_c_n)\n",
        "        output = layer_output\n",
        "        \n",
        "        h_n = torch.stack(h_n, 0)\n",
        "        c_n = torch.stack(c_n, 0)\n",
        "        return output, (h_n, c_n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo9lwgCRukcg",
        "colab_type": "text"
      },
      "source": [
        "## Test LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxf5P8elufU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoding: utf-8\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "@author: huayuxiu\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import data_processing\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.stats import logistic\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "\n",
        "\n",
        "data_path = '/content/drive/My Drive/TFM/RCLSTM/data/traffic-1-2.npy'\n",
        "time_window = 100\n",
        "hidden_size = 300\n",
        "max_iter = 5\n",
        "\n",
        "#download data\n",
        "data = np.load(data_path)\n",
        "new_data = []\n",
        "for x in data:\n",
        "    if x > 0:\n",
        "        new_data.append(np.log10(x))\n",
        "    else:\n",
        "        new_data.append(0.001)\n",
        "new_data = np.array(new_data)\n",
        "new_data = new_data[new_data>2.5]\n",
        "data = new_data[new_data<6]\n",
        "\n",
        "max_data = np.max(data)\n",
        "min_data = np.min(data)\n",
        "data = (data-min_data)/(max_data-min_data)\n",
        "\n",
        "df = pd.DataFrame({'temp':data})\n",
        "# define function for create N lags\n",
        "def create_lags(df, N):\n",
        "    for i in range(N):\n",
        "        df['Lag' + str(i+1)] = df.temp.shift(i+1)\n",
        "    return df\n",
        "\n",
        "# create time-windows lags\n",
        "df = create_lags(df,time_window)\n",
        "\n",
        "# the first 1000 days will have missing values. can't use them.\n",
        "df = df.dropna()\n",
        "\n",
        "# create X and y\n",
        "y = df.temp.values\n",
        "X = df.iloc[:, 1:].values\n",
        "\n",
        "# Train on 70% of the data\n",
        "train_idx = int(len(df) * .5)\n",
        "\n",
        "# create train and test data\n",
        "train_X, train_Y, test_X, test_Y = X[:train_idx], y[:train_idx], X[train_idx:], y[train_idx:]\n",
        "\n",
        "test_X = test_X.reshape(-1, time_window, 1)\n",
        "test_Y = test_Y.reshape(-1, 1)\n",
        "print(len(test_X))\n",
        "\n",
        "# Rebuild the LSTM calculation model and replace the matrix calculations in the model with the calculation of the sparse matrix\n",
        "def rnn_cell(connectivity, input, hx, cell_weight_hh, cell_weight_ih, cell_bias):\n",
        "    output = []\n",
        "    for time in range(max_time):\n",
        "        h, c = hx\n",
        "        x = input[max_time-1-time]\n",
        "        # print(h.shape)\n",
        "        # print(cell_weight_hh.shape)\n",
        "        # print(cell_bias.shape)\n",
        "\n",
        "        # If the model is sparse, use the sparse matrix calculation method\n",
        "        #if connectivity == '1' or connectivity == '10':\n",
        "        #    wh_b = h * cell_weight_hh + cell_bias\n",
        "        #    wi = x * cell_weight_ih\n",
        "\n",
        "        # If the model is not sparse, use conventional calculation methods\n",
        "        #else:\n",
        "        wh_b = np.dot(h, cell_weight_hh) + cell_bias\n",
        "        wi = np.dot(x, cell_weight_ih)\n",
        "        f, i, o, g = np.split(wh_b + wi, 4, axis=1)\n",
        "        c_next = np.multiply(logistic.cdf(f), c) + np.multiply(logistic.cdf(i), np.tanh(g))\n",
        "        h_next = np.multiply(logistic.cdf(o), np.tanh(c))\n",
        "        hx = (h_next, c_next)\n",
        "        output.append(h_next)\n",
        "        # if time % 50 == 0:\n",
        "        #     print(time)\n",
        "    output = np.stack(output, 0)\n",
        "    return output, hx\n",
        "\n",
        "def rnn(input, hx, cell_weight_hh, cell_weight_ih, cell_bias):\n",
        "    input_ = np.transpose(input, [1, 0, 2])\n",
        "    h_n = []\n",
        "    c_n = []\n",
        "    layer_output = None\n",
        "    for layer in range(3):\n",
        "        layer_output, (layer_h_n, layer_c_n) = rnn_cell(\n",
        "            input=input_, hx=hx, cell_weight_hh=cell_weight_hh[layer],\n",
        "            cell_weight_ih=cell_weight_ih[layer], cell_bias=cell_bias[layer])\n",
        "        print(layer_output.shape)\n",
        "        input_ = layer_output\n",
        "        h_n.append(layer_h_n)\n",
        "        c_n.append(layer_c_n)\n",
        "    output = layer_output\n",
        "    h_n = np.stack(h_n, 0)\n",
        "    c_n = np.stack(c_n, 0)\n",
        "    return output, (h_n, c_n)\n",
        "\n",
        "# rclstm_model = torch.load('/home/hyx/Pytorch/Traffic_prediction/RNN/model/lstm-model.pt')\n",
        "# rclstm_model_dict = rclstm_model.state_dict()\n",
        "# print(rclstm_model_dict.keys())\n",
        "\n",
        "RMSE = []\n",
        "run_time = []\n",
        "save_dir = './model/'\n",
        "for connectivity in ['1', '10', '20', '30', '40', '50', '60', '70', '80', '90', '100']:\n",
        "    save_path = save_dir + connectivity\n",
        "    temp = []\n",
        "\n",
        "    for save in range(max_iter):\n",
        "        # print(os.path.join(save_path, str(save)+'.pt'))\n",
        "        lstm_model = torch.load(os.path.join(save_path, str(save)+'.pt'))\n",
        "        lstm_model_dict = lstm_model.state_dict()\n",
        "\n",
        "        model_dict = lstm_model_dict\n",
        "\n",
        "        cell_weight_ih_0 = model_dict['rnn.cell_0.weight_ih'].cpu().data.numpy()\n",
        "        cell_weight_hh_0 = model_dict['rnn.cell_0.weight_hh'].cpu().data.numpy()\n",
        "        cell_bias_0 = model_dict['rnn.cell_0.bias'].cpu().data.numpy()\n",
        "\n",
        "        # If the weight matrix is sparse, the matrix is changed to a sparse matrix representation.\n",
        "        if connectivity == '1' or connectivity == '10':\n",
        "            cell_weight_ih_0 = csr_matrix(cell_weight_ih_0)\n",
        "            cell_weight_hh_0 = csr_matrix(cell_weight_hh_0)\n",
        "            print('sparse matrices')\n",
        "\n",
        "        fc2_weight = model_dict['fc2.weight'].cpu().data.numpy()\n",
        "        fc2_bias = model_dict['fc2.bias'].cpu().data.numpy()\n",
        "\n",
        "\n",
        "        num_samples, max_time, _ = test_X.shape\n",
        "\n",
        "        h = np.zeros((num_samples, hidden_size))\n",
        "        c = np.zeros((num_samples, hidden_size))\n",
        "        hx = (h, c)\n",
        "\n",
        "        t1 = time.time()\n",
        "\n",
        "        input_ = np.transpose(test_X, [1, 0, 2])\n",
        "\n",
        "        h_n, _ = rnn_cell(connectivity, input_, hx, cell_weight_hh_0, cell_weight_ih_0, cell_bias_0)\n",
        "\n",
        "        logit = np.dot(h_n[-1], fc2_weight.T) + fc2_bias\n",
        "        t2 = time.time()\n",
        "\n",
        "        prediction = logit\n",
        "        actual = test_Y\n",
        "\n",
        "        RMSELoss = np.sqrt(np.mean((actual - prediction)**2))\n",
        "\n",
        "        # calculating time\n",
        "        total_time = t2 - t1\n",
        "        print('test RMSEloss is %.4f, total computating time is %.5f' % (RMSELoss, t2-t1))\n",
        "\n",
        "        # Take the best performance of max-iter models as the final result\n",
        "        if save == 0:\n",
        "            temp = [RMSELoss, total_time]\n",
        "        else:\n",
        "            temp[0] = RMSELoss if RMSELoss < temp[0] else temp[0]\n",
        "            temp[1] = total_time if total_time > temp[1] else temp[1]\n",
        "\n",
        "\n",
        "    RMSE.append(float('%.4f' % temp[0]))\n",
        "    run_time.append(float('%.4f' % temp[1]))\n",
        "\n",
        "print(RMSE, run_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alMCOAE4VK9u",
        "colab_type": "text"
      },
      "source": [
        "## Theory\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Min-Max Scaling **\n",
        "\n",
        "Feature scaling is used to bring all values into the range [0,1]. This is also called unity-based normalization. \n",
        "\n",
        "**Variable Class**\n",
        "\n",
        "The Variable class is the main component of this autograd system in PyTorch. This Variable class wraps a tensor, and allows automatic gradient computation on the tensor when the .backward() function is called (more on this later). The object contains the data of the tensor, the gradient of the tensor (once computed with respect to some other value i.e. the loss) and also contains a reference to whatever function created the variable (if it is a user created function, this reference will be null).\n",
        "\n",
        "`x = Variable(torch_tensor, requires_grad=True)  `\n",
        "\n",
        "require_grad = True specify that this variable requires a gradient. If we were using this in a neural network, this would mean that this Variable would be trainable. If we set this flag to False, the Variable would not be trained.\n",
        "\n",
        "**CUDA semantics**\n",
        "\n",
        "torch.cuda is used to set up and run CUDA operations. It keeps track of the currently selected GPU, and all CUDA tensors you allocate will by default be created on that device\n",
        "\n",
        "** [Lags in Time Series Analysis](https://math.stackexchange.com/questions/2548314/what-is-lag-in-a-time-series)**\n",
        "\n",
        "Lag is essentially delay. Just as correlation shows how much two timeseries are similar, autocorrelation describes how similar the time series is with itself.\n",
        "\n",
        "Lags are very useful in time series analysis because of a phenomenon called autocorrelation, which is a tendency for the values within a time series to be correlated with previous copies of itself. One benefit to autocorrelation is that we can identify patterns within the time series, which helps in determining seasonality, the tendency for patterns to repeat at periodic frequencies. \n",
        "\n",
        "Finally, lags and autocorrelation are central to numerous forecasting models that incorporate autoregression, regressing a time series using previous values of itself. Autoregression is the basis for one of the most widely used forecasting techniques, the autoregressive integrated moving average model or ARIMA for short. Possibly the most widely used tool for forecasting, the forecast package by Rob Hyndman, implements ARIMA (and a number of other forecast modeling techniques). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ7lR1NkafmY",
        "colab_type": "text"
      },
      "source": [
        "# Offloading MEC\n",
        "\n",
        "We consider an MEC network composed by one edge server, one wireless access point (AP), and N WDs, denoted by a set $N = {1, 2, . . . , N}$\n",
        "\n",
        "\n",
        "\n",
        "The AP and the edge server is connected by an optical fiber, whose transmission delay can be ignored.\n",
        "\n",
        "Each WD has multiple tasks to be processed locally or be offloaded to the edge server via the AP. Without loss of generality, we assume that each WD has M independent tasks, denoted by a set $M = {1, 2, . . . , M}$. Denote $d_{nm}$ as the workload of the m-th task of user n. Each WD n can determine whether to offload its task m to the edge server or not, and the offloading decision is denoted by a binary variable $x_{nm} ∈ {0, 1}$. Specifically, $x_{nm}=1$ denotes that user n decides to offload its task m to the edge server, and $d_{nm}=0$ means that user n decides to execute its task m locally. The detailed operations of edge computing and local computing are illustrated as follows.\n",
        " \n",
        "**Edge Commputing**\n",
        "\n",
        "When a task is offloaded to the edge server, the WD n transmits its workload  dnm to the AP via wireless channels which is further forwarded to and processed at the edge server. We neglect the energy consumption and delay when the edge server transmits the computing results back to WDs. We denote $E_{nm}^{t}$\n",
        " as the energy consumed by WDs for uploading its workload to the edge server and model the energy cost for data process at edge server as a linear function of workload $d_{nm}$. Specifically, we denote the total cost for user $n$ to offload its task $m$ to the edge server as: \n",
        " \n",
        " $$E_{nm}^{c} = E_{nm}^{t} + \\alpha d_{nm}$$\n",
        "\n",
        "where $α$ is the weight of the energy consumption at edge server. When $α = 0$, we only consider the energy consumption at WDs. Notice that the cost $E_{nm}^{c}$ includes the energy consumptions for sending the task and the server’s utility cost for executing this task. We next model the delay in computation offloading. Specifically, we use $c_{n}$ to denote the allocated bandwidth to user $n$ for transmitting its offloaded task to the edge server. Therefore,** the transmission delay** when user $n$ offloads its task $m$ to the edge server is given by:\n",
        "\n",
        "$$T_{nm}^{t}=\\frac{ d_{nm}}{c_{n}}$$\n",
        "\n",
        "In addition, **the edge processing delay is given by** :\n",
        "\n",
        "$$T_{nm}^{c}=\\frac{ d_{nm}}{f^{c}}$$\n",
        "\n",
        "where we denote ${f^{c}}$ as the edge processing rate. In\n",
        "summary, given the offloading decisions ${x_{nm}}$, the total\n",
        "delay of user n when it executes MEC can be given by:\n",
        "\n",
        "$$T_{nm}^{c}={ \\sum_{m=1}^{M}(T_{nm}^{c} + T_{nm}^{t}){x_{nm}}, \\forall n}$$\n",
        "\n",
        "We assume that the edge server can only start to process user n’s task m after this task is completely received by the edge, and the edge-server can only start to send back the output data after the entire task m is completed.\n",
        "\n",
        "**Local Computing**\n",
        "\n",
        "We next model the case when user n decides to execute its task locally. Specifically, we use $e_{n}^{l}$ to denote the local energy consumption per data bit of user n. Thus, user n’s energy consumption for executing its task m locally is given by:\n",
        "\n",
        " $$E_{nm}^{l}=d_{nm}e_{n}^{l}$$\n",
        "\n",
        "Meanwhile, we denote user n’s local processing time per data bit as ${t^{l}}$. As a result, the total processing time for user n to execute its task m is given by:\n",
        "\n",
        "$$T_{nm}^{l}=d_{nm}{t^{l}}$$\n",
        "\n",
        "Thus, given user n’s offloading decision  ${x_{nm}}$, the total delay for user n to finish its tasks locally is given by:\n",
        "\n",
        "$$T_{n}^{l}={ \\sum_{m=1}^{M}T_{nm}^{l}(1 - {x_{nm})}, \\forall n}$$\n",
        "\n",
        "**Problem formulation**\n",
        "\n",
        "To minimize both the total delay finishing all users’ tasks and the corresponding energy consumptions, we first introduce a system utility *Q(d, x, c)* defined as the weighted sum of energy consumption and task completion delay, as:\n",
        "\n",
        "$$Q(d,x,c)=\\sum_{m=1}^{M}(\\sum_{m=1}^{M}(E_{nm}^{l}(1-x_{nm})+ E_{nm}x_{nm}) + \\beta max [T_{n}^{l},T_{n}^{c}])$$\n",
        "\n",
        "where $d=d_{nm}|n ∈ N, m ∈ M, x = x_{nm}|x ∈ N, m ∈ M, x =c_{n}|x ∈ N$ and $\\beta$ denotes the weight of energy comsumption and task completition.\n",
        "\n",
        "Now, we formulate the first optimization problem (P1) to minimize $Q(d,x,c)$ by jointly optimizing each user n's offloading decisions $x_{nm}$ and the bandwidth allocations $c_{n}$ for user n's task transmission, which is expressed as follows:\n",
        "\n",
        "\n",
        " $$(P1): Q^{*}(d) = \\underset{x,c}{minimize} \\hspace{0.5cm} Q(d,x,c)$$\n",
        "\n",
        "$$\\hspace{3.2cm} subject \\hspace{0.1cm} to: \\sum_{n=1}^{N}c_{n}\\leq C,$$\n",
        "\n",
        "$$\\hspace{1.7cm}c_{n}\\geqslant 0, \\forall n ∈ N, $$\n",
        "\n",
        "$$\\hspace{1.1cm}x_{nm} ∈ [0,1]. $$\n",
        "\n",
        "\n",
        "Here, constraint $\\sum_{n=1}^{N}c_{n}\\leq C$  means that the total uplink bandwidth allocation for all users cannot exceed the maximum bandwidth C. The allocated bandwidth for each user cn is\n",
        "either 0 or positive. \n",
        "\n",
        "The optimization problem (P1) is a mixed-integer programming problem, which is difficult to solve in general. We will study an approximate algorithm based on deep learning to efficiently and effectively solve it.\n",
        "\n"
      ]
    }
  ]
}